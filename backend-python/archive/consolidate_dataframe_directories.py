#!/usr/bin/env python3
"""
DataFrame Directory Consolidation Manager
========================================

Consolidates dataframe_data/ and dataframe_data_compatible/ into a single unified structure.

Current Problem:
- dataframe_data/ has all 5 entities (properties, tenants, workorders, transactions, documents)  
- dataframe_data_compatible/ has only properties (for frontend compatibility)
- This creates confusion and duplicate maintenance

Solution:
- Merge both into unified dataframe_data/ with all entities
- Update all services to use single data directory
- Maintain frontend compatibility within the unified structure
"""

import pandas as pd
import shutil
from pathlib import Path
from datetime import datetime
import json

class DataFrameDirectoryConsolidator:
    def __init__(self):
        self.backend_dir = Path(__file__).parent
        self.main_dir = self.backend_dir / "dataframe_data"
        self.compatible_dir = self.backend_dir / "dataframe_data_compatible"
        self.backup_dir = self.backend_dir / "consolidation_backups"
        
    def analyze_current_state(self):
        """Analyze what's in both directories"""
        print("üîç ANALYZING CURRENT DATAFRAME DIRECTORIES")
        print("=" * 50)
        
        # Check main directory
        if self.main_dir.exists():
            main_files = list(self.main_dir.glob("*.csv"))
            print(f"üìä dataframe_data/ - {len(main_files)} files:")
            for file in main_files:
                df = pd.read_csv(file)
                print(f"   ‚úÖ {file.name}: {len(df)} records")
        else:
            print("‚ùå dataframe_data/ not found")
            
        # Check compatible directory  
        if self.compatible_dir.exists():
            compatible_files = list(self.compatible_dir.glob("*.csv"))
            print(f"üîó dataframe_data_compatible/ - {len(compatible_files)} files:")
            for file in compatible_files:
                df = pd.read_csv(file)
                print(f"   ‚úÖ {file.name}: {len(df)} records")
        else:
            print("‚ùå dataframe_data_compatible/ not found")
            
    def create_backups(self):
        """Create backups before consolidation"""
        print("\nüíæ CREATING BACKUPS")
        print("=" * 30)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.backup_dir.mkdir(exist_ok=True)
        
        # Backup main directory
        if self.main_dir.exists():
            main_backup = self.backup_dir / f"dataframe_data_backup_{timestamp}"
            shutil.copytree(self.main_dir, main_backup)
            print(f"‚úÖ Backed up dataframe_data/ ‚Üí {main_backup}")
            
        # Backup compatible directory
        if self.compatible_dir.exists():
            compatible_backup = self.backup_dir / f"dataframe_data_compatible_backup_{timestamp}"
            shutil.copytree(self.compatible_dir, compatible_backup)
            print(f"‚úÖ Backed up dataframe_data_compatible/ ‚Üí {compatible_backup}")
            
    def consolidate_properties(self):
        """Merge properties.csv from both directories"""
        print("\nüîÑ CONSOLIDATING PROPERTIES DATA")
        print("=" * 40)
        
        main_props_file = self.main_dir / "properties.csv"
        compatible_props_file = self.compatible_dir / "properties.csv"
        
        # Load both files
        main_props = pd.read_csv(main_props_file) if main_props_file.exists() else pd.DataFrame()
        compatible_props = pd.read_csv(compatible_props_file) if compatible_props_file.exists() else pd.DataFrame()
        
        print(f"üìä dataframe_data/properties.csv: {len(main_props)} records")
        print(f"üîó dataframe_data_compatible/properties.csv: {len(compatible_props)} records")
        
        if len(compatible_props) > 0:
            # Merge data (compatible_props is more recent/clean)
            if len(main_props) == 0:
                # Use compatible data as base
                merged_props = compatible_props
                print("‚úÖ Using dataframe_data_compatible/ as base (main was empty)")
            else:
                # Merge intelligently - compatible has priority
                merged_props = compatible_props.copy()
                print("‚úÖ Using dataframe_data_compatible/ as authoritative source")
            
            # Save merged data back to main directory
            merged_props.to_csv(main_props_file, index=False)
            print(f"üíæ Saved {len(merged_props)} properties to dataframe_data/properties.csv")
            
        return len(merged_props) if 'merged_props' in locals() else 0
        
    def update_service_references(self):
        """Update all service files to use unified dataframe_data directory"""
        print("\nüîß UPDATING SERVICE REFERENCES")
        print("=" * 40)
        
        files_to_update = [
            'app_frontend_compatible.py',
            'test_frontend_compatibility.py',
            'data_json_csv_integrator.py',
            'continuous_sync.py',
            'duplicate_prevention_manager.py',
            'integrate_frontend_backend.py'
        ]
        
        updates_made = 0
        
        for filename in files_to_update:
            file_path = self.backend_dir / filename
            if file_path.exists():
                try:
                    # Read file content
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # Replace references
                    original_content = content
                    content = content.replace('dataframe_data_compatible', 'dataframe_data')
                    
                    if content != original_content:
                        # Write back updated content
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(content)
                        print(f"   ‚úÖ Updated {filename}")
                        updates_made += 1
                    else:
                        print(f"   ‚ûñ {filename} (no changes needed)")
                        
                except Exception as e:
                    print(f"   ‚ùå Error updating {filename}: {e}")
        
        print(f"\nüìä Updated {updates_made} files to use unified dataframe_data/")
        
    def remove_compatible_directory(self):
        """Remove the now-redundant dataframe_data_compatible directory"""
        print("\nüóëÔ∏è REMOVING REDUNDANT DIRECTORY")
        print("=" * 40)
        
        if self.compatible_dir.exists():
            try:
                shutil.rmtree(self.compatible_dir)
                print("‚úÖ Removed dataframe_data_compatible/ directory")
                print("   (Backup preserved in consolidation_backups/)")
            except Exception as e:
                print(f"‚ùå Error removing directory: {e}")
        else:
            print("‚ûñ dataframe_data_compatible/ already removed")
            
    def generate_consolidation_report(self, properties_count):
        """Generate consolidation report"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'consolidation_summary': {
                'unified_directory': 'dataframe_data/',
                'removed_directory': 'dataframe_data_compatible/',
                'properties_consolidated': properties_count,
                'services_updated': True,
                'backups_created': True
            },
            'final_structure': {
                'dataframe_data/': {
                    'properties.csv': 'Unified properties data',
                    'tenants.csv': 'All tenant records',
                    'workorders.csv': 'All work orders', 
                    'transactions.csv': 'All transactions',
                    'documents.csv': 'All document records'
                }
            },
            'benefits': [
                'Single source of truth for all data',
                'No more directory confusion',
                'Simplified maintenance',
                'All entities in one place',
                'Frontend compatibility maintained'
            ]
        }
        
        report_path = self.backup_dir / "consolidation_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
            
        print(f"üìÑ Consolidation report saved: {report_path}")
        return report
        
    def run_consolidation(self):
        """Run complete consolidation process"""
        print("üöÄ DATAFRAME DIRECTORY CONSOLIDATION")
        print("=" * 50)
        print("Unifying dataframe_data/ and dataframe_data_compatible/...")
        print()
        
        # 1. Analyze current state
        self.analyze_current_state()
        
        # 2. Create backups
        self.create_backups()
        
        # 3. Consolidate properties
        properties_count = self.consolidate_properties()
        
        # 4. Update service references
        self.update_service_references()
        
        # 5. Remove compatible directory
        self.remove_compatible_directory()
        
        # 6. Generate report
        report = self.generate_consolidation_report(properties_count)
        
        print("\nüéâ CONSOLIDATION COMPLETE!")
        print("=" * 30)
        print("‚úÖ Single unified dataframe_data/ directory")
        print("‚úÖ All services updated to use unified structure")
        print("‚úÖ Frontend compatibility maintained")
        print("‚úÖ Complete backups created")
        print()
        print("üéØ Result: Simplified, unified DataFrame storage!")
        
        return report

if __name__ == '__main__':
    consolidator = DataFrameDirectoryConsolidator()
    consolidator.run_consolidation()